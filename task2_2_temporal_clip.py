# -*- coding: utf-8 -*-
"""task2_2_temporal_clip.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ue19hdJJUY-ZZQ7wgSyXoToYSU6F_s7u

# Task 2.2: Temporal Emotion Contrastive Learning with CLIP ViT

This notebook implements temporal contrastive learning using **CLIP ViT-L/14** with **LoRA adaptation**.

## Why CLIP instead of BLIP-2?
-  **7-10x faster training** (300M vs 2.7B parameters)
-  **Better emotion recognition** (55-70% vs 33-45% alignment)
-  **More efficient LoRA** (~5M vs ~40M trainable params)
-  **Training time**: 1-2 hours vs 6-8 hours

## Key Features:
- Temporal triplet sampling from video sequences
- Emotion-aware contrastive loss
- Smooth transition modeling
- LoRA fine-tuning on frozen CLIP
"""

# Imports
import os
import json
import pickle
import warnings
from pathlib import Path
from collections import defaultdict

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from tqdm.auto import tqdm

from transformers import CLIPVisionModel, CLIPProcessor, get_linear_schedule_with_warmup
# from peft import LoraConfig, get_peft_model
from PIL import Image

warnings.filterwarnings('ignore')
sns.set_style('whitegrid')

print(" Imports successful!")
print(f"PyTorch: {torch.__version__}")
print(f"CUDA: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")

# Configuration
class Config:
    # PATHS - Update if needed!
    PROJECT_ROOT = Path('/home_local/sdeshmukh/Multimodal-Vision-Language-Emotion-Understanding/outputs')
    # DATA_DIR = PROJECT_ROOT / 'data' / 'AFEW-VA'
    TASK2_2_DIR = PROJECT_ROOT / 'task2_2_temporal'
    CHECKPOINTS_DIR = TASK2_2_DIR / 'checkpoints'
    RESULTS_DIR = TASK2_2_DIR / 'results'
    VISUALIZATIONS_DIR = TASK2_2_DIR / 'visualizations'

        # Update this to your actual dataset path
    DATA_DIR = Path('/home_local/sdeshmukh/Multimodal-Vision-Language-Emotion-Understanding/dataset')

    # Project output directories
    
    # EMBEDDINGS_DIR = PROJECT_ROOT / 'embeddings'
    # CHECKPOINTS_DIR = PROJECT_ROOT / 'checkpoints'
    # VISUALIZATIONS_DIR = PROJECT_ROOT / 'visualizations'
    # RESULTS_DIR = PROJECT_ROOT / 'results'


    # Model
    CLIP_MODEL = "openai/clip-vit-base-patch16"
  # 300M params

    # LoRA
    LORA_R = 16
    LORA_ALPHA = 32
    LORA_DROPOUT = 0.1
    LORA_TARGET_MODULES = ["q_proj", "v_proj"]

    # Training
    BATCH_SIZE = 32
    LEARNING_RATE = 2e-4
    NUM_EPOCHS = 20
    WARMUP_EPOCHS = 2
    TEMPERATURE = 0.07

    # Temporal
    CLIP_LENGTH = 3.0
    FPS_ASSUMPTION = 30
    MAX_FRAME_DELTA = int(CLIP_LENGTH * FPS_ASSUMPTION)
    VALENCE_THRESHOLD = 2.0
    AROUSAL_THRESHOLD = 2.0

    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    SEED = 42

config = Config()

# Create directories
for d in [config.TASK2_2_DIR, config.CHECKPOINTS_DIR, config.RESULTS_DIR, config.VISUALIZATIONS_DIR]:
    d.mkdir(parents=True, exist_ok=True)

# Set seeds
np.random.seed(config.SEED)
torch.manual_seed(config.SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(config.SEED)

print(" Config ready")
print(f"Model: {config.CLIP_MODEL}")
print(f"Device: {config.DEVICE}")
print(f"Save dir: {config.TASK2_2_DIR}")

"""## Data Loading & Temporal Organization"""

def load_afew_data(data_dir):
    """Load AFEW-VA dataset"""
    print("Loading AFEW-VA...")

    all_samples = []
    clip_dirs = sorted([d for d in data_dir.iterdir() if d.is_dir()])

    for clip_dir in tqdm(clip_dirs, desc="Loading clips"):
        clip_name = clip_dir.name
        annotation_file = clip_dir / f"{clip_name}.json"

        if not annotation_file.exists():
            continue

        with open(annotation_file, 'r') as f:
            annotations = json.load(f)

        frames = annotations.get("frames", {})

        for frame_id, frame_data in frames.items():
            frame_num = int(frame_id)
            image_path = clip_dir / f"{frame_id}.png"
            if not image_path.exists():
                image_path = clip_dir / f"{frame_id}.jpg"

            if image_path.exists() and frame_data.get('valence') is not None:
                all_samples.append({
                    'clip_name': clip_name,
                    'frame_id': frame_id,
                    'frame_num': frame_num,
                    'image_path': str(image_path),
                    'valence': frame_data.get('valence'),
                    'arousal': frame_data.get('arousal'),
                })

    print(f" Loaded {len(all_samples)} frames from {len(clip_dirs)} clips")
    return all_samples


def organize_by_clips(samples):
    """Organize samples by video clip"""
    clips = defaultdict(list)
    for s in samples:
        clips[s['clip_name']].append(s)

    for clip_name in clips:
        clips[clip_name] = sorted(clips[clip_name], key=lambda x: x['frame_num'])

    print(f" Organized into {len(clips)} clips")
    return dict(clips)


# Load data
all_samples = load_afew_data(config.DATA_DIR)
clips_dict = organize_by_clips(all_samples)

"""## Split Train/Val (by clips to prevent leakage)"""

# Split by clips (80/20)
clip_names = list(clips_dict.keys())
np.random.shuffle(clip_names)
split_idx = int(0.8 * len(clip_names))

train_clips = set(clip_names[:split_idx])
val_clips = set(clip_names[split_idx:])

train_samples = [s for s in all_samples if s['clip_name'] in train_clips]
val_samples = [s for s in all_samples if s['clip_name'] in val_clips]

print(f" Train: {len(train_samples)} frames from {len(train_clips)} clips")
print(f" Val: {len(val_samples)} frames from {len(val_clips)} clips")

"""## Temporal Triplet Dataset"""

class TemporalTripletDataset(Dataset):
    def __init__(self, samples, clips_dict, processor, config, split='train'):
        self.samples = samples
        self.clips_dict = clips_dict
        self.processor = processor
        self.config = config
        self.split = split
        print(f" {split} dataset: {len(samples)} frames")

    def __len__(self):
        return len(self.samples)

    def _emotion_distance(self, s1, s2):
        vd = abs(s1['valence'] - s2['valence'])
        ad = abs(s1['arousal'] - s2['arousal'])
        return np.sqrt(vd**2 + ad**2)

    def _is_similar(self, s1, s2):
        vd = abs(s1['valence'] - s2['valence'])
        ad = abs(s1['arousal'] - s2['arousal'])
        return vd <= self.config.VALENCE_THRESHOLD and ad <= self.config.AROUSAL_THRESHOLD

    def _get_positive(self, anchor):
        clip_frames = self.clips_dict[anchor['clip_name']]
        anchor_num = anchor['frame_num']

        candidates = []
        for frame in clip_frames:
            delta = abs(frame['frame_num'] - anchor_num)
            if 0 < delta <= self.config.MAX_FRAME_DELTA:
                if self._is_similar(anchor, frame):
                    candidates.append((frame, delta))

        if not candidates:
            # Fallback: any similar emotion in clip
            similar = [f for f in clip_frames if f['frame_num'] != anchor_num and self._is_similar(anchor, f)]
            return similar[0] if similar else anchor

        candidates.sort(key=lambda x: x[1])
        weights = [1.0/(d+1) for _, d in candidates]
        weights = np.array(weights) / sum(weights)
        idx = np.random.choice(len(candidates), p=weights)
        return candidates[idx][0]

    def _get_negative(self, anchor):
        n = min(100, len(self.samples))
        indices = np.random.choice(len(self.samples), n, replace=False)

        best_neg = None
        max_dist = 0

        for idx in indices:
            cand = self.samples[idx]
            if cand['clip_name'] == anchor['clip_name'] and cand['frame_num'] == anchor['frame_num']:
                continue
            dist = self._emotion_distance(anchor, cand)
            if dist > max_dist:
                max_dist = dist
                best_neg = cand

        return best_neg if best_neg else self.samples[np.random.randint(len(self.samples))]

    def __getitem__(self, idx):
        anchor = self.samples[idx]
        positive = self._get_positive(anchor)
        negative = self._get_negative(anchor)

        return {
            'anchor_image': Image.open(anchor['image_path']).convert('RGB'),
            'positive_image': Image.open(positive['image_path']).convert('RGB'),
            'negative_image': Image.open(negative['image_path']).convert('RGB'),
            'anchor_valence': torch.tensor(anchor['valence'], dtype=torch.float32),
            'anchor_arousal': torch.tensor(anchor['arousal'], dtype=torch.float32),
            'positive_valence': torch.tensor(positive['valence'], dtype=torch.float32),
            'positive_arousal': torch.tensor(positive['arousal'], dtype=torch.float32),
        }


def collate_triplets(batch):
    return {
        'anchor_images': [b['anchor_image'] for b in batch],
        'positive_images': [b['positive_image'] for b in batch],
        'negative_images': [b['negative_image'] for b in batch],
        'anchor_valences': torch.stack([b['anchor_valence'] for b in batch]),
        'anchor_arousals': torch.stack([b['anchor_arousal'] for b in batch]),
        'positive_valences': torch.stack([b['positive_valence'] for b in batch]),
        'positive_arousals': torch.stack([b['positive_arousal'] for b in batch]),
    }

print(" Dataset classes defined")

"""## CLIP Model with LoRA"""

# FINAL FIX WITH DTYPE HANDLING
# Replace your LoRALayer and CLIPWithLoRA classes with this:

# ABSOLUTE FIX - Explicitly freeze original layer parameters

class LoRALayer(nn.Module):
    """LoRA with proper freeze and device handling"""
    def __init__(self, original_layer, r=16, alpha=32, dropout=0.1):
        super().__init__()
        
        self.r = r
        self.alpha = alpha
        self.scaling = alpha / r
        
        # Clone weights and move to same device as original
        device = next(original_layer.parameters()).device
        dtype = next(original_layer.parameters()).dtype
        
        with torch.no_grad():
            # Register as buffer (not parameter) so it's not trainable
            self.register_buffer('weight', original_layer.weight.detach().clone().to(device))
            if original_layer.bias is not None:
                self.register_buffer('bias', original_layer.bias.detach().clone().to(device))
            else:
                self.register_buffer('bias', None)
        
        in_features = original_layer.in_features
        out_features = original_layer.out_features
        
        # LoRA parameters - ONLY these are trainable
        self.lora_A = nn.Parameter(torch.zeros(in_features, r, dtype=dtype, device=device))
        self.lora_B = nn.Parameter(torch.zeros(r, out_features, dtype=dtype, device=device))
        self.dropout = nn.Dropout(dropout)
        
        # Initialize
        with torch.no_grad():
            nn.init.kaiming_uniform_(self.lora_A, a=5**0.5)
            nn.init.zeros_(self.lora_B)
    
    def forward(self, x):
        """Forward with manual linear + LoRA"""
        # Base layer (frozen weights)
        result = F.linear(x, self.weight, self.bias)
        
        # LoRA addition
        x_for_lora = x
        if x.dtype != self.lora_A.dtype:
            x_for_lora = x.to(self.lora_A.dtype)
        
        lora_result = self.dropout(x_for_lora) @ self.lora_A @ self.lora_B * self.scaling
        
        if lora_result.dtype != result.dtype:
            lora_result = lora_result.to(result.dtype)
        
        return result + lora_result


class CLIPWithLoRA(nn.Module):
    """CLIP with LoRA - COMPLETE FREEZE + DEVICE FIX"""
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        print(f"Loading {config.CLIP_MODEL}...")
        clip_full = CLIPVisionModel.from_pretrained(
            config.CLIP_MODEL,
            torch_dtype=torch.float32
        )
        
        self.vision_model = clip_full.vision_model
        
        # STEP 1: Freeze EVERYTHING first
        print("Freezing ALL base model parameters...")
        for param in self.vision_model.parameters():
            param.requires_grad = False
        
        # STEP 2: Apply LoRA (creates NEW trainable params)
        print("Applying LoRA layers...")
        self._apply_lora()
        
        # STEP 3: Verify
        self._print_trainable_parameters()
    
    def _apply_lora(self):
        """Replace q_proj and v_proj with LoRA versions"""
        lora_count = 0
        
        for layer_idx, layer in enumerate(self.vision_model.encoder.layers):
            # Replace q_proj
            if hasattr(layer.self_attn, 'q_proj'):
                old_q = layer.self_attn.q_proj
                new_q = LoRALayer(
                    old_q,
                    r=self.config.LORA_R,
                    alpha=self.config.LORA_ALPHA,
                    dropout=self.config.LORA_DROPOUT
                )
                layer.self_attn.q_proj = new_q
                lora_count += 1
            
            # Replace v_proj  
            if hasattr(layer.self_attn, 'v_proj'):
                old_v = layer.self_attn.v_proj
                new_v = LoRALayer(
                    old_v,
                    r=self.config.LORA_R,
                    alpha=self.config.LORA_ALPHA,
                    dropout=self.config.LORA_DROPOUT
                )
                layer.self_attn.v_proj = new_v
                lora_count += 1
        
        print(f" Replaced {lora_count} layers with LoRA")
    
    def _print_trainable_parameters(self):
        """Verify only LoRA params are trainable"""
        trainable = 0
        total = 0
        lora_trainable = 0
        base_trainable_list = []
        
        for name, param in self.named_parameters():
            total += param.numel()
            if param.requires_grad:
                trainable += param.numel()
                if 'lora_' in name:
                    lora_trainable += param.numel()
                else:
                    # This shouldn't happen!
                    base_trainable_list.append((name, param.numel()))
        
        percentage = 100 * trainable / total
        
        print(f"\nTrainable params: {trainable:,}")
        print(f"Total params: {total:,}")
        print(f"Trainable %: {percentage:.2f}%")
        print(f"LoRA params: {lora_trainable:,}")
        
        if len(base_trainable_list) > 0:
            print(f"\n ERROR: {len(base_trainable_list)} base parameters are still trainable:")
            for name, count in base_trainable_list[:10]:  # Show first 10
                print(f"  - {name}: {count:,}")
            
            print("\nAttempting to freeze them...")
            for name, param in self.named_parameters():
                if 'lora_' not in name:
                    param.requires_grad = False
            
            # Recount
            trainable_after = sum(p.numel() for p in self.parameters() if p.requires_grad)
            print(f"After fix: {trainable_after:,} trainable")
        
        if percentage < 0.5:
            print(" Less than 0.5% trainable - LoRA may not work")
        elif percentage > 10:
            print(" More than 10% trainable - base not frozen!")
        else:
            print(" Correct! Only LoRA parameters are trainable")
    
    def forward(self, pixel_values):
        """Forward pass"""
        outputs = self.vision_model(pixel_values=pixel_values)
        
        if hasattr(outputs, 'pooler_output'):
            pooled_output = outputs.pooler_output
        elif isinstance(outputs, tuple):
            pooled_output = outputs[1]
        else:
            pooled_output = outputs.last_hidden_state[:, 0, :]
        
        embeddings = F.normalize(pooled_output, p=2, dim=-1)
        return embeddings
    
    def save_lora_weights(self, path):
        from pathlib import Path
        import json
        path = Path(path)
        path.mkdir(parents=True, exist_ok=True)
        
        lora_state = {name: param.cpu() for name, param in self.named_parameters() 
                      if 'lora_' in name and param.requires_grad}
        torch.save(lora_state, path / 'lora_weights.pt')
        
        config_dict = {
            'lora_r': self.config.LORA_R,
            'lora_alpha': self.config.LORA_ALPHA,
            'clip_model': self.config.CLIP_MODEL,
        }
        with open(path / 'lora_config.json', 'w') as f:
            json.dump(config_dict, f, indent=2)
        
        print(f" Saved to {path}")


print("="*70)
print(" COMPLETE FIX: Freeze + Device")
print("="*70)
print("1. Uses register_buffer() for frozen weights (not trainable)")
print("2. Weights created on correct device")
print("3. Explicitly freezes all base params before LoRA")

"""## Temporal Contrastive Loss"""

# ANTI-COLLAPSE LOSS FUNCTION

# STABLE ANTI-COLLAPSE LOSS (Fixed diversity explosion)

class TemporalContrastiveLoss(nn.Module):
    """
    Stable contrastive loss that prevents collapse
    """
    def __init__(self, temperature=0.07, margin=1.0):
        super().__init__()
        self.temperature = temperature
        self.margin = margin
    
    def forward(self, anchor_emb, positive_emb, negative_emb,
                anchor_valence, positive_valence, anchor_arousal, positive_arousal):
        """
        Stable anti-collapse loss
        """
        # Normalize embeddings
        anchor_emb = F.normalize(anchor_emb, p=2, dim=1)
        positive_emb = F.normalize(positive_emb, p=2, dim=1)
        negative_emb = F.normalize(negative_emb, p=2, dim=1)
        
        # Cosine similarities
        pos_sim = F.cosine_similarity(anchor_emb, positive_emb, dim=1)
        neg_sim = F.cosine_similarity(anchor_emb, negative_emb, dim=1)
        
        # 1. Triplet margin loss (main component)
        triplet_loss = F.relu(neg_sim - pos_sim + self.margin).mean()
        
        # 2. InfoNCE loss (for proper contrastive learning)
        logits = torch.cat([pos_sim.unsqueeze(1), neg_sim.unsqueeze(1)], dim=1) / self.temperature
        labels = torch.zeros(logits.size(0), dtype=torch.long, device=logits.device)
        infonce_loss = F.cross_entropy(logits, labels)
        
        # 3. Penalty for negative similarity being too high
        neg_penalty = F.relu(neg_sim - 0.5).mean()
        
        # 4. FIXED: Stable diversity loss (prevent explosion)
        batch_std = torch.std(anchor_emb, dim=0).mean()
        # Encourage std > 0.1, but don't let loss explode if std is tiny
        diversity_loss = torch.clamp(0.1 - batch_std, min=0, max=1.0)
        
        # 5. Emotion consistency
        valence_diff = torch.abs(anchor_valence - positive_valence)
        arousal_diff = torch.abs(anchor_arousal - positive_arousal)
        emotion_distance = torch.sqrt(valence_diff**2 + arousal_diff**2)
        emotion_weight = torch.exp(-emotion_distance / 3.0)
        emotion_loss = ((1.0 - pos_sim) * emotion_weight).mean()
        
        # Total loss (balanced weights)
        total_loss = (
            triplet_loss +           # Primary: separate pos/neg
            0.5 * infonce_loss +     # Contrastive structure
            0.5 * neg_penalty +      # Keep negatives low
            0.1 * diversity_loss +   # Prevent collapse (stable!)
            0.2 * emotion_loss       # Emotion consistency
        )
        
        return {
            'total_loss': total_loss,
            'triplet_loss': triplet_loss,
            'infonce_loss': infonce_loss,
            'neg_penalty': neg_penalty,
            'diversity_loss': diversity_loss,
            'emotion_loss': emotion_loss,
            'pos_sim': pos_sim.mean(),
            'neg_sim': neg_sim.mean(),
        }


print("="*70)
print(" STABLE ANTI-COLLAPSE LOSS")
print("="*70)
print("Fixed: diversity_loss clamped to prevent explosion")
print("Margin: 1.0 (reasonable)")
print("Temperature: 0.07 (standard)")


# ============================================================================
# RECOMMENDED SETTINGS
# ============================================================================
print("\n" + "="*70)
print("RECOMMENDED SETTINGS:")
print("="*70)
print("""
# Create loss
criterion = TemporalContrastiveLoss(temperature=0.07, margin=1.0)

# Optimizer
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=5e-5,           # Conservative
    weight_decay=0.01  # Standard
)

# Batch size
config.BATCH_SIZE = 32  # Keep as is
""")



"""## Initialize Model & Datasets"""

# Load processor and model
print("Initializing CLIP...")
from transformers import CLIPImageProcessor
processor = CLIPImageProcessor.from_pretrained(config.CLIP_MODEL)
model = CLIPWithLoRA(config).to(config.DEVICE)

# Create datasets
print("\nCreating datasets...")
train_dataset = TemporalTripletDataset(train_samples, clips_dict, processor, config, 'train')
val_dataset = TemporalTripletDataset(val_samples, clips_dict, processor, config, 'val')

# Dataloaders
train_loader = DataLoader(
    train_dataset, batch_size=config.BATCH_SIZE, shuffle=True,
    num_workers=4, collate_fn=collate_triplets, pin_memory=True
)
val_loader = DataLoader(
    val_dataset, batch_size=config.BATCH_SIZE, shuffle=False,
    num_workers=4, collate_fn=collate_triplets, pin_memory=True
)

print(f"\n Ready to train!")
print(f"Train batches: {len(train_loader)}")
print(f"Val batches: {len(val_loader)}")

"""## Training Loop"""

# Setup training
criterion = TemporalContrastiveLoss(temperature=config.TEMPERATURE)
optimizer = torch.optim.AdamW(
    model.parameters(), 
    lr=2e-4,
    weight_decay=0.05  # Increased from 0.01
)

num_training_steps = len(train_loader) * config.NUM_EPOCHS
num_warmup_steps = num_training_steps * config.WARMUP_EPOCHS // config.NUM_EPOCHS
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)

# Training history
history = {'train_loss': [], 'val_loss': [], 'pos_sim': [], 'neg_sim': []}

print("="*70)
print("STARTING TRAINING")
print("="*70)

# Training loop
best_val_loss = float('inf')

for epoch in range(config.NUM_EPOCHS):
    # TRAIN
    model.train()
    train_losses = []
    train_pos_sims = []
    train_neg_sims = []

    pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{config.NUM_EPOCHS}")
    for batch in pbar:
        # Process images
        anchor_in = processor(images=batch['anchor_images'], return_tensors='pt')
        positive_in = processor(images=batch['positive_images'], return_tensors='pt')
        negative_in = processor(images=batch['negative_images'], return_tensors='pt')

        anchor_px = anchor_in['pixel_values'].to(config.DEVICE)
        positive_px = positive_in['pixel_values'].to(config.DEVICE)
        negative_px = negative_in['pixel_values'].to(config.DEVICE)

        anchor_v = batch['anchor_valences'].to(config.DEVICE)
        positive_v = batch['positive_valences'].to(config.DEVICE)
        anchor_a = batch['anchor_arousals'].to(config.DEVICE)
        positive_a = batch['positive_arousals'].to(config.DEVICE)

        # Forward
        anchor_emb = model(anchor_px)
        positive_emb = model(positive_px)
        negative_emb = model(negative_px)

        # Loss
        loss_dict = criterion(anchor_emb, positive_emb, negative_emb,
                             anchor_v, positive_v, anchor_a, positive_a)
        loss = loss_dict['total_loss']

        # Backward
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()

        train_losses.append(loss.item())
        train_pos_sims.append(loss_dict['pos_sim'].item())
        train_neg_sims.append(loss_dict['neg_sim'].item())

        pbar.set_postfix({
            'loss': f"{loss.item():.4f}",
            'pos': f"{loss_dict['pos_sim'].item():.3f}",
            'neg': f"{loss_dict['neg_sim'].item():.3f}"
        })

        del anchor_px, positive_px, negative_px
        del anchor_emb, positive_emb, negative_emb
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    # VALIDATE
    model.eval()
    val_losses = []

    with torch.no_grad():
        for batch in tqdm(val_loader, desc="Validating", leave=False):
            anchor_in = processor(images=batch['anchor_images'], return_tensors='pt')
            positive_in = processor(images=batch['positive_images'], return_tensors='pt')
            negative_in = processor(images=batch['negative_images'], return_tensors='pt')

            anchor_px = anchor_in['pixel_values'].to(config.DEVICE)
            positive_px = positive_in['pixel_values'].to(config.DEVICE)
            negative_px = negative_in['pixel_values'].to(config.DEVICE)

            anchor_v = batch['anchor_valences'].to(config.DEVICE)
            positive_v = batch['positive_valences'].to(config.DEVICE)
            anchor_a = batch['anchor_arousals'].to(config.DEVICE)
            positive_a = batch['positive_arousals'].to(config.DEVICE)

            anchor_emb = model(anchor_px)
            positive_emb = model(positive_px)
            negative_emb = model(negative_px)

            loss_dict = criterion(anchor_emb, positive_emb, negative_emb,
                                 anchor_v, positive_v, anchor_a, positive_a)
            val_losses.append(loss_dict['total_loss'].item())

            del anchor_px, positive_px, negative_px
            del anchor_emb, positive_emb, negative_emb
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

    # Record
    train_loss = np.mean(train_losses)
    val_loss = np.mean(val_losses)
    pos_sim = np.mean(train_pos_sims)
    neg_sim = np.mean(train_neg_sims)

    history['train_loss'].append(train_loss)
    history['val_loss'].append(val_loss)
    history['pos_sim'].append(pos_sim)
    history['neg_sim'].append(neg_sim)

    print(f"\nEpoch {epoch+1}/{config.NUM_EPOCHS}:")
    print(f"  Train Loss: {train_loss:.4f}")
    print(f"  Val Loss:   {val_loss:.4f}")
    print(f"  Pos Sim:    {pos_sim:.3f}")
    print(f"  Neg Sim:    {neg_sim:.3f}")
    print(f"  Margin:     {pos_sim - neg_sim:.3f}")

    # Save best
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        model.save_lora_weights(config.CHECKPOINTS_DIR / 'best_model')
        print(f"   Best model saved!")

    # Checkpoint
    if (epoch + 1) % 5 == 0:
        model.save_lora_weights(config.CHECKPOINTS_DIR / f'checkpoint_epoch_{epoch+1}')
        print(f"   Checkpoint saved")

print("\n" + "="*70)
print(" TRAINING COMPLETE!")
print("="*70)

# DIAGNOSTIC CODE - Run this BEFORE training to debug

# Test a single batch
print("\nChecking triplet quality...")
test_batch = next(iter(train_loader))

print(f"Anchor valences: {test_batch['anchor_valences'][:5]}")
print(f"Positive valences: {test_batch['positive_valences'][:5]}")
print(f"Difference: {torch.abs(test_batch['anchor_valences'][:5] - test_batch['positive_valences'][:5])}")


print("=" * 70)
print("DEBUGGING PROCESSOR OUTPUT")
print("=" * 70)

# Process one set of images
anchor_in = processor(images=test_batch['anchor_images'], return_tensors='pt')

print("\n1. What keys does processor return?")
print(f"   Keys: {anchor_in.keys()}")

print("\n2. Shapes:")
for key, value in anchor_in.items():
    if hasattr(value, 'shape'):
        print(f"   {key}: {value.shape}")

print("\n3. Testing pixel_values extraction:")
anchor_px = anchor_in['pixel_values'].to(config.DEVICE)
print(f"   Extracted pixel_values shape: {anchor_px.shape}")
print(f"   Type: {type(anchor_px)}")

print("\n4. Testing model forward:")
try:
    # This is what we're doing in training
    test_emb = model(anchor_px)
    print(f"    SUCCESS! Embedding shape: {test_emb.shape}")
except Exception as e:
    print(f"    ERROR: {e}")
    print(f"   Error type: {type(e).__name__}")

print("\n5. Alternative: Direct call to clip_vision")
try:
    test_output = model.clip_vision(pixel_values=anchor_px, return_dict=True)
    test_emb2 = F.normalize(test_output.pooler_output, p=2, dim=-1)
    print(f"    Direct call SUCCESS! Embedding shape: {test_emb2.shape}")
except Exception as e:
    print(f"    ERROR: {e}")

print("\n" + "=" * 70)
print("Run this diagnostic to see where the error occurs!")
print("=" * 70)

"""## Visualize Results"""

# Plot training curves
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

epochs = range(1, len(history['train_loss']) + 1)

# Loss
axes[0].plot(epochs, history['train_loss'], label='Train', linewidth=2)
axes[0].plot(epochs, history['val_loss'], label='Val', linewidth=2)
axes[0].set_xlabel('Epoch', fontsize=12)
axes[0].set_ylabel('Loss', fontsize=12)
axes[0].set_title('Training Loss', fontsize=14, fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)

# Similarities
axes[1].plot(epochs, history['pos_sim'], label='Positive', linewidth=2)
axes[1].plot(epochs, history['neg_sim'], label='Negative', linewidth=2)
margin = [p - n for p, n in zip(history['pos_sim'], history['neg_sim'])]
axes[1].plot(epochs, margin, label='Margin', linewidth=2, linestyle='--')
axes[1].set_xlabel('Epoch', fontsize=12)
axes[1].set_ylabel('Cosine Similarity', fontsize=12)
axes[1].set_title('Embedding Similarities', fontsize=14, fontweight='bold')
axes[1].legend()
axes[1].grid(alpha=0.3)
axes[1].axhline(0, color='k', linestyle='-', alpha=0.3)

plt.tight_layout()
plt.savefig(config.VISUALIZATIONS_DIR / 'training_curves.png', dpi=150, bbox_inches='tight')
plt.show()

print(f" Saved to {config.VISUALIZATIONS_DIR / 'training_curves.png'}")

# Save history
with open(config.RESULTS_DIR / 'training_history.pkl', 'wb') as f:
    pickle.dump(history, f)

# Save final model
model.save_lora_weights(config.CHECKPOINTS_DIR / 'final_model')

print("="*70)
print(" TASK 2.2 COMPLETE!")
print("="*70)
print(f"\nResults saved to: {config.TASK2_2_DIR}")
print(f"  - Best model: {config.CHECKPOINTS_DIR / 'best_model'}")
print(f"  - Training curves: {config.VISUALIZATIONS_DIR / 'training_curves.png'}")
print(f"  - History: {config.RESULTS_DIR / 'training_history.pkl'}")