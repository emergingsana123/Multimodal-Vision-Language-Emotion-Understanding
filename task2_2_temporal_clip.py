# -*- coding: utf-8 -*-
"""task2_2_temporal_clip.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ue19hdJJUY-ZZQ7wgSyXoToYSU6F_s7u

# Task 2.2: Temporal Emotion Contrastive Learning with CLIP ViT

This notebook implements temporal contrastive learning using **CLIP ViT-L/14** with **LoRA adaptation**.

## Why CLIP instead of BLIP-2?
- âœ… **7-10x faster training** (300M vs 2.7B parameters)
- âœ… **Better emotion recognition** (55-70% vs 33-45% alignment)
- âœ… **More efficient LoRA** (~5M vs ~40M trainable params)
- âœ… **Training time**: 1-2 hours vs 6-8 hours

## Key Features:
- Temporal triplet sampling from video sequences
- Emotion-aware contrastive loss
- Smooth transition modeling
- LoRA fine-tuning on frozen CLIP
"""

# Imports
import os
import json
import pickle
import warnings
from pathlib import Path
from collections import defaultdict

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from tqdm.auto import tqdm

from transformers import CLIPVisionModel, CLIPProcessor, get_linear_schedule_with_warmup
# from peft import LoraConfig, get_peft_model
from PIL import Image

warnings.filterwarnings('ignore')
sns.set_style('whitegrid')

print("âœ… Imports successful!")
print(f"PyTorch: {torch.__version__}")
print(f"CUDA: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")

# Configuration
class Config:
    # PATHS - Update if needed!
    PROJECT_ROOT = Path('/home_local/sdeshmukh/Multimodal-Vision-Language-Emotion-Understanding/outputs')
    # DATA_DIR = PROJECT_ROOT / 'data' / 'AFEW-VA'
    TASK2_2_DIR = PROJECT_ROOT / 'task2_2_temporal'
    CHECKPOINTS_DIR = TASK2_2_DIR / 'checkpoints'
    RESULTS_DIR = TASK2_2_DIR / 'results'
    VISUALIZATIONS_DIR = TASK2_2_DIR / 'visualizations'

        # Update this to your actual dataset path
    DATA_DIR = Path('/home_local/sdeshmukh/Multimodal-Vision-Language-Emotion-Understanding/dataset')

    # Project output directories
    
    # EMBEDDINGS_DIR = PROJECT_ROOT / 'embeddings'
    # CHECKPOINTS_DIR = PROJECT_ROOT / 'checkpoints'
    # VISUALIZATIONS_DIR = PROJECT_ROOT / 'visualizations'
    # RESULTS_DIR = PROJECT_ROOT / 'results'


    # Model
    CLIP_MODEL = "openai/clip-vit-base-patch16"
  # 300M params

    # LoRA
    LORA_R = 16
    LORA_ALPHA = 32
    LORA_DROPOUT = 0.1
    LORA_TARGET_MODULES = ["q_proj", "v_proj"]

    # Training
    BATCH_SIZE = 32
    LEARNING_RATE = 1e-4
    NUM_EPOCHS = 20
    WARMUP_EPOCHS = 2
    TEMPERATURE = 0.07

    # Temporal
    CLIP_LENGTH = 3.0
    FPS_ASSUMPTION = 30
    MAX_FRAME_DELTA = int(CLIP_LENGTH * FPS_ASSUMPTION)
    VALENCE_THRESHOLD = 2.0
    AROUSAL_THRESHOLD = 2.0

    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    SEED = 42

config = Config()

# Create directories
for d in [config.TASK2_2_DIR, config.CHECKPOINTS_DIR, config.RESULTS_DIR, config.VISUALIZATIONS_DIR]:
    d.mkdir(parents=True, exist_ok=True)

# Set seeds
np.random.seed(config.SEED)
torch.manual_seed(config.SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(config.SEED)

print("âœ… Config ready")
print(f"Model: {config.CLIP_MODEL}")
print(f"Device: {config.DEVICE}")
print(f"Save dir: {config.TASK2_2_DIR}")

"""## Data Loading & Temporal Organization"""

def load_afew_data(data_dir):
    """Load AFEW-VA dataset"""
    print("Loading AFEW-VA...")

    all_samples = []
    clip_dirs = sorted([d for d in data_dir.iterdir() if d.is_dir()])

    for clip_dir in tqdm(clip_dirs, desc="Loading clips"):
        clip_name = clip_dir.name
        annotation_file = clip_dir / f"{clip_name}.json"

        if not annotation_file.exists():
            continue

        with open(annotation_file, 'r') as f:
            annotations = json.load(f)

        frames = annotations.get("frames", {})

        for frame_id, frame_data in frames.items():
            frame_num = int(frame_id)
            image_path = clip_dir / f"{frame_id}.png"
            if not image_path.exists():
                image_path = clip_dir / f"{frame_id}.jpg"

            if image_path.exists() and frame_data.get('valence') is not None:
                all_samples.append({
                    'clip_name': clip_name,
                    'frame_id': frame_id,
                    'frame_num': frame_num,
                    'image_path': str(image_path),
                    'valence': frame_data.get('valence'),
                    'arousal': frame_data.get('arousal'),
                })

    print(f"âœ… Loaded {len(all_samples)} frames from {len(clip_dirs)} clips")
    return all_samples


def organize_by_clips(samples):
    """Organize samples by video clip"""
    clips = defaultdict(list)
    for s in samples:
        clips[s['clip_name']].append(s)

    for clip_name in clips:
        clips[clip_name] = sorted(clips[clip_name], key=lambda x: x['frame_num'])

    print(f"âœ… Organized into {len(clips)} clips")
    return dict(clips)


# Load data
all_samples = load_afew_data(config.DATA_DIR)
clips_dict = organize_by_clips(all_samples)

"""## Split Train/Val (by clips to prevent leakage)"""

# Split by clips (80/20)
clip_names = list(clips_dict.keys())
np.random.shuffle(clip_names)
split_idx = int(0.8 * len(clip_names))

train_clips = set(clip_names[:split_idx])
val_clips = set(clip_names[split_idx:])

train_samples = [s for s in all_samples if s['clip_name'] in train_clips]
val_samples = [s for s in all_samples if s['clip_name'] in val_clips]

print(f"âœ… Train: {len(train_samples)} frames from {len(train_clips)} clips")
print(f"âœ… Val: {len(val_samples)} frames from {len(val_clips)} clips")

"""## Temporal Triplet Dataset"""

class TemporalTripletDataset(Dataset):
    def __init__(self, samples, clips_dict, processor, config, split='train'):
        self.samples = samples
        self.clips_dict = clips_dict
        self.processor = processor
        self.config = config
        self.split = split
        print(f"âœ… {split} dataset: {len(samples)} frames")

    def __len__(self):
        return len(self.samples)

    def _emotion_distance(self, s1, s2):
        vd = abs(s1['valence'] - s2['valence'])
        ad = abs(s1['arousal'] - s2['arousal'])
        return np.sqrt(vd**2 + ad**2)

    def _is_similar(self, s1, s2):
        vd = abs(s1['valence'] - s2['valence'])
        ad = abs(s1['arousal'] - s2['arousal'])
        return vd <= self.config.VALENCE_THRESHOLD and ad <= self.config.AROUSAL_THRESHOLD

    def _get_positive(self, anchor):
        clip_frames = self.clips_dict[anchor['clip_name']]
        anchor_num = anchor['frame_num']

        candidates = []
        for frame in clip_frames:
            delta = abs(frame['frame_num'] - anchor_num)
            if 0 < delta <= self.config.MAX_FRAME_DELTA:
                if self._is_similar(anchor, frame):
                    candidates.append((frame, delta))

        if not candidates:
            # Fallback: any similar emotion in clip
            similar = [f for f in clip_frames if f['frame_num'] != anchor_num and self._is_similar(anchor, f)]
            return similar[0] if similar else anchor

        candidates.sort(key=lambda x: x[1])
        weights = [1.0/(d+1) for _, d in candidates]
        weights = np.array(weights) / sum(weights)
        idx = np.random.choice(len(candidates), p=weights)
        return candidates[idx][0]

    def _get_negative(self, anchor):
        n = min(100, len(self.samples))
        indices = np.random.choice(len(self.samples), n, replace=False)

        best_neg = None
        max_dist = 0

        for idx in indices:
            cand = self.samples[idx]
            if cand['clip_name'] == anchor['clip_name'] and cand['frame_num'] == anchor['frame_num']:
                continue
            dist = self._emotion_distance(anchor, cand)
            if dist > max_dist:
                max_dist = dist
                best_neg = cand

        return best_neg if best_neg else self.samples[np.random.randint(len(self.samples))]

    def __getitem__(self, idx):
        anchor = self.samples[idx]
        positive = self._get_positive(anchor)
        negative = self._get_negative(anchor)

        return {
            'anchor_image': Image.open(anchor['image_path']).convert('RGB'),
            'positive_image': Image.open(positive['image_path']).convert('RGB'),
            'negative_image': Image.open(negative['image_path']).convert('RGB'),
            'anchor_valence': torch.tensor(anchor['valence'], dtype=torch.float32),
            'anchor_arousal': torch.tensor(anchor['arousal'], dtype=torch.float32),
            'positive_valence': torch.tensor(positive['valence'], dtype=torch.float32),
            'positive_arousal': torch.tensor(positive['arousal'], dtype=torch.float32),
        }


def collate_triplets(batch):
    return {
        'anchor_images': [b['anchor_image'] for b in batch],
        'positive_images': [b['positive_image'] for b in batch],
        'negative_images': [b['negative_image'] for b in batch],
        'anchor_valences': torch.stack([b['anchor_valence'] for b in batch]),
        'anchor_arousals': torch.stack([b['anchor_arousal'] for b in batch]),
        'positive_valences': torch.stack([b['positive_valence'] for b in batch]),
        'positive_arousals': torch.stack([b['positive_arousal'] for b in batch]),
    }

print("âœ… Dataset classes defined")

"""## CLIP Model with LoRA"""

# FINAL FIX WITH DTYPE HANDLING
# Replace your LoRALayer and CLIPWithLoRA classes with this:

class LoRALayer(nn.Module):
    """Manual LoRA implementation with proper dtype handling"""
    def __init__(self, original_layer, r=16, alpha=32, dropout=0.1):
        super().__init__()
        self.original_layer = original_layer
        self.r = r
        self.alpha = alpha
        self.scaling = alpha / r

        # Freeze original weights
        for param in self.original_layer.parameters():
            param.requires_grad = False

        # Get dimensions
        in_features = original_layer.in_features
        out_features = original_layer.out_features

        # CRITICAL FIX: Match dtype of original layer
        # Check if original layer uses float16
        original_dtype = next(original_layer.parameters()).dtype

        # Create LoRA matrices with matching dtype
        self.lora_A = nn.Parameter(torch.zeros(in_features, r, dtype=original_dtype))
        self.lora_B = nn.Parameter(torch.zeros(r, out_features, dtype=original_dtype))
        self.dropout = nn.Dropout(dropout)

        # Initialize (with correct dtype)
        with torch.no_grad():
            nn.init.kaiming_uniform_(self.lora_A, a=5**0.5)
            nn.init.zeros_(self.lora_B)

    def forward(self, x):
        """Forward with dtype-safe operations"""
        # Original forward
        result = self.original_layer(x)

        # LoRA forward - ensure x matches LoRA dtype
        x_dtype = x.dtype
        lora_A_dtype = self.lora_A.dtype

        # Convert if needed
        if x_dtype != lora_A_dtype:
            x = x.to(lora_A_dtype)

        # Compute LoRA result
        lora_result = self.dropout(x) @ self.lora_A @ self.lora_B * self.scaling

        # Convert back if needed
        if lora_result.dtype != result.dtype:
            lora_result = lora_result.to(result.dtype)

        return result + lora_result


class CLIPWithLoRA(nn.Module):
    """CLIP Vision Model with Manual LoRA - DTYPE SAFE VERSION"""

    def __init__(self, config):
        super().__init__()
        self.config = config

        print(f"Loading {config.CLIP_MODEL}...")
        clip_full = CLIPVisionModel.from_pretrained(
            config.CLIP_MODEL,
            torch_dtype=torch.float16 if config.DEVICE.type == 'cuda' else torch.float32
        )

        # Extract vision model
        self.vision_model = clip_full.vision_model

        print("Applying manual LoRA with dtype matching...")
        self._apply_lora()
        self._print_trainable_parameters()

    def _apply_lora(self):
        """Apply LoRA to q_proj and v_proj"""
        lora_count = 0
        for layer in self.vision_model.encoder.layers:
            if hasattr(layer.self_attn, 'q_proj'):
                layer.self_attn.q_proj = LoRALayer(
                    layer.self_attn.q_proj,
                    r=self.config.LORA_R,
                    alpha=self.config.LORA_ALPHA,
                    dropout=self.config.LORA_DROPOUT
                )
                lora_count += 1

            if hasattr(layer.self_attn, 'v_proj'):
                layer.self_attn.v_proj = LoRALayer(
                    layer.self_attn.v_proj,
                    r=self.config.LORA_R,
                    alpha=self.config.LORA_ALPHA,
                    dropout=self.config.LORA_DROPOUT
                )
                lora_count += 1

        print(f"âœ… Applied LoRA to {lora_count} layers")

    def _print_trainable_parameters(self):
        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)
        total = sum(p.numel() for p in self.parameters())
        print(f"Trainable: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)")

    def forward(self, pixel_values):
        """Forward pass with proper dtype handling"""
        # Call vision_model
        outputs = self.vision_model(pixel_values=pixel_values)

        # Extract pooler output
        if hasattr(outputs, 'pooler_output'):
            pooled_output = outputs.pooler_output
        elif isinstance(outputs, tuple):
            pooled_output = outputs[1]
        else:
            pooled_output = outputs.last_hidden_state[:, 0, :]

        # Normalize
        embeddings = F.normalize(pooled_output, p=2, dim=-1)

        return embeddings

    def save_lora_weights(self, path):
        from pathlib import Path
        import json
        path = Path(path)
        path.mkdir(parents=True, exist_ok=True)

        lora_state = {name: param.cpu() for name, param in self.named_parameters()
                      if 'lora_' in name and param.requires_grad}
        torch.save(lora_state, path / 'lora_weights.pt')

        config_dict = {
            'lora_r': self.config.LORA_R,
            'lora_alpha': self.config.LORA_ALPHA,
            'clip_model': self.config.CLIP_MODEL,
        }
        with open(path / 'lora_config.json', 'w') as f:
            json.dump(config_dict, f, indent=2)

        print(f"âœ… Saved to {path}")


print("="*70)
print("âœ… DTYPE-SAFE VERSION")
print("="*70)
print("LoRA parameters will match the model's dtype (float16/float32)")

# model = CLIPWithLoRA(config).to(config.DEVICE)

# test_batch = next(iter(train_loader))
# test_inputs = processor(images=test_batch['anchor_images'][:4], return_tensors='pt')
# test_pixels = test_inputs['pixel_values'].to(config.DEVICE)

# with torch.no_grad():
#     test_emb = model(test_pixels)
#     print(f"âœ… SUCCESS! Shape: {test_emb.shape}")
#     print(f"Embedding dtype: {test_emb.dtype}")

"""## Temporal Contrastive Loss"""

class TemporalContrastiveLoss(nn.Module):
    def __init__(self, temperature=0.07, margin=0.5):
        super().__init__()
        self.temperature = temperature
        self.margin = margin

    def forward(self, anchor_emb, positive_emb, negative_emb,
                anchor_valence, positive_valence, anchor_arousal, positive_arousal):
        # Cosine similarities
        pos_sim = F.cosine_similarity(anchor_emb, positive_emb, dim=1)
        neg_sim = F.cosine_similarity(anchor_emb, negative_emb, dim=1)

        # Triplet loss
        triplet_loss = F.relu((neg_sim - pos_sim) / self.temperature + self.margin).mean()

        # Emotion consistency
        valence_diff = torch.abs(anchor_valence - positive_valence)
        arousal_diff = torch.abs(anchor_arousal - positive_arousal)
        emotion_distance = torch.sqrt(valence_diff**2 + arousal_diff**2)
        emotion_weight = torch.exp(-emotion_distance / 5.0)
        consistency_loss = ((1.0 - pos_sim) * emotion_weight).mean()

        total_loss = triplet_loss + 0.5 * consistency_loss

        return {
            'total_loss': total_loss,
            'triplet_loss': triplet_loss,
            'consistency_loss': consistency_loss,
            'pos_sim': pos_sim.mean(),
            'neg_sim': neg_sim.mean(),
        }

print("âœ… Loss function defined")

"""## Initialize Model & Datasets"""

# Load processor and model
print("Initializing CLIP...")
from transformers import CLIPImageProcessor
processor = CLIPImageProcessor.from_pretrained(config.CLIP_MODEL)
model = CLIPWithLoRA(config).to(config.DEVICE)

# Create datasets
print("\nCreating datasets...")
train_dataset = TemporalTripletDataset(train_samples, clips_dict, processor, config, 'train')
val_dataset = TemporalTripletDataset(val_samples, clips_dict, processor, config, 'val')

# Dataloaders
train_loader = DataLoader(
    train_dataset, batch_size=config.BATCH_SIZE, shuffle=True,
    num_workers=4, collate_fn=collate_triplets, pin_memory=True
)
val_loader = DataLoader(
    val_dataset, batch_size=config.BATCH_SIZE, shuffle=False,
    num_workers=4, collate_fn=collate_triplets, pin_memory=True
)

print(f"\nâœ… Ready to train!")
print(f"Train batches: {len(train_loader)}")
print(f"Val batches: {len(val_loader)}")

"""## Training Loop"""

# Setup training
criterion = TemporalContrastiveLoss(temperature=config.TEMPERATURE)
optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=0.01)

num_training_steps = len(train_loader) * config.NUM_EPOCHS
num_warmup_steps = num_training_steps * config.WARMUP_EPOCHS // config.NUM_EPOCHS
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)

# Training history
history = {'train_loss': [], 'val_loss': [], 'pos_sim': [], 'neg_sim': []}

print("="*70)
print("STARTING TRAINING")
print("="*70)

# Training loop
best_val_loss = float('inf')

for epoch in range(config.NUM_EPOCHS):
    # TRAIN
    model.train()
    train_losses = []
    train_pos_sims = []
    train_neg_sims = []

    pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{config.NUM_EPOCHS}")
    for batch in pbar:
        # Process images
        anchor_in = processor(images=batch['anchor_images'], return_tensors='pt')
        positive_in = processor(images=batch['positive_images'], return_tensors='pt')
        negative_in = processor(images=batch['negative_images'], return_tensors='pt')

        anchor_px = anchor_in['pixel_values'].to(config.DEVICE)
        positive_px = positive_in['pixel_values'].to(config.DEVICE)
        negative_px = negative_in['pixel_values'].to(config.DEVICE)

        anchor_v = batch['anchor_valences'].to(config.DEVICE)
        positive_v = batch['positive_valences'].to(config.DEVICE)
        anchor_a = batch['anchor_arousals'].to(config.DEVICE)
        positive_a = batch['positive_arousals'].to(config.DEVICE)

        # Forward
        anchor_emb = model(anchor_px)
        positive_emb = model(positive_px)
        negative_emb = model(negative_px)

        # Loss
        loss_dict = criterion(anchor_emb, positive_emb, negative_emb,
                             anchor_v, positive_v, anchor_a, positive_a)
        loss = loss_dict['total_loss']

        # Backward
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()

        train_losses.append(loss.item())
        train_pos_sims.append(loss_dict['pos_sim'].item())
        train_neg_sims.append(loss_dict['neg_sim'].item())

        pbar.set_postfix({
            'loss': f"{loss.item():.4f}",
            'pos': f"{loss_dict['pos_sim'].item():.3f}",
            'neg': f"{loss_dict['neg_sim'].item():.3f}"
        })

        del anchor_px, positive_px, negative_px
        del anchor_emb, positive_emb, negative_emb
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    # VALIDATE
    model.eval()
    val_losses = []

    with torch.no_grad():
        for batch in tqdm(val_loader, desc="Validating", leave=False):
            anchor_in = processor(images=batch['anchor_images'], return_tensors='pt')
            positive_in = processor(images=batch['positive_images'], return_tensors='pt')
            negative_in = processor(images=batch['negative_images'], return_tensors='pt')

            anchor_px = anchor_in['pixel_values'].to(config.DEVICE)
            positive_px = positive_in['pixel_values'].to(config.DEVICE)
            negative_px = negative_in['pixel_values'].to(config.DEVICE)

            anchor_v = batch['anchor_valences'].to(config.DEVICE)
            positive_v = batch['positive_valences'].to(config.DEVICE)
            anchor_a = batch['anchor_arousals'].to(config.DEVICE)
            positive_a = batch['positive_arousals'].to(config.DEVICE)

            anchor_emb = model(anchor_px)
            positive_emb = model(positive_px)
            negative_emb = model(negative_px)

            loss_dict = criterion(anchor_emb, positive_emb, negative_emb,
                                 anchor_v, positive_v, anchor_a, positive_a)
            val_losses.append(loss_dict['total_loss'].item())

            del anchor_px, positive_px, negative_px
            del anchor_emb, positive_emb, negative_emb
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

    # Record
    train_loss = np.mean(train_losses)
    val_loss = np.mean(val_losses)
    pos_sim = np.mean(train_pos_sims)
    neg_sim = np.mean(train_neg_sims)

    history['train_loss'].append(train_loss)
    history['val_loss'].append(val_loss)
    history['pos_sim'].append(pos_sim)
    history['neg_sim'].append(neg_sim)

    print(f"\nEpoch {epoch+1}/{config.NUM_EPOCHS}:")
    print(f"  Train Loss: {train_loss:.4f}")
    print(f"  Val Loss:   {val_loss:.4f}")
    print(f"  Pos Sim:    {pos_sim:.3f}")
    print(f"  Neg Sim:    {neg_sim:.3f}")
    print(f"  Margin:     {pos_sim - neg_sim:.3f}")

    # Save best
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        model.save_lora_weights(config.CHECKPOINTS_DIR / 'best_model')
        print(f"  âœ… Best model saved!")

    # Checkpoint
    if (epoch + 1) % 5 == 0:
        model.save_lora_weights(config.CHECKPOINTS_DIR / f'checkpoint_epoch_{epoch+1}')
        print(f"  âœ… Checkpoint saved")

print("\n" + "="*70)
print("âœ… TRAINING COMPLETE!")
print("="*70)

# DIAGNOSTIC CODE - Run this BEFORE training to debug

# Test a single batch
test_batch = next(iter(train_loader))

print("=" * 70)
print("DEBUGGING PROCESSOR OUTPUT")
print("=" * 70)

# Process one set of images
anchor_in = processor(images=test_batch['anchor_images'], return_tensors='pt')

print("\n1. What keys does processor return?")
print(f"   Keys: {anchor_in.keys()}")

print("\n2. Shapes:")
for key, value in anchor_in.items():
    if hasattr(value, 'shape'):
        print(f"   {key}: {value.shape}")

print("\n3. Testing pixel_values extraction:")
anchor_px = anchor_in['pixel_values'].to(config.DEVICE)
print(f"   Extracted pixel_values shape: {anchor_px.shape}")
print(f"   Type: {type(anchor_px)}")

print("\n4. Testing model forward:")
try:
    # This is what we're doing in training
    test_emb = model(anchor_px)
    print(f"   âœ… SUCCESS! Embedding shape: {test_emb.shape}")
except Exception as e:
    print(f"   âŒ ERROR: {e}")
    print(f"   Error type: {type(e).__name__}")

print("\n5. Alternative: Direct call to clip_vision")
try:
    test_output = model.clip_vision(pixel_values=anchor_px, return_dict=True)
    test_emb2 = F.normalize(test_output.pooler_output, p=2, dim=-1)
    print(f"   âœ… Direct call SUCCESS! Embedding shape: {test_emb2.shape}")
except Exception as e:
    print(f"   âŒ ERROR: {e}")

print("\n" + "=" * 70)
print("Run this diagnostic to see where the error occurs!")
print("=" * 70)

"""## Visualize Results"""

# Plot training curves
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

epochs = range(1, len(history['train_loss']) + 1)

# Loss
axes[0].plot(epochs, history['train_loss'], label='Train', linewidth=2)
axes[0].plot(epochs, history['val_loss'], label='Val', linewidth=2)
axes[0].set_xlabel('Epoch', fontsize=12)
axes[0].set_ylabel('Loss', fontsize=12)
axes[0].set_title('Training Loss', fontsize=14, fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)

# Similarities
axes[1].plot(epochs, history['pos_sim'], label='Positive', linewidth=2)
axes[1].plot(epochs, history['neg_sim'], label='Negative', linewidth=2)
margin = [p - n for p, n in zip(history['pos_sim'], history['neg_sim'])]
axes[1].plot(epochs, margin, label='Margin', linewidth=2, linestyle='--')
axes[1].set_xlabel('Epoch', fontsize=12)
axes[1].set_ylabel('Cosine Similarity', fontsize=12)
axes[1].set_title('Embedding Similarities', fontsize=14, fontweight='bold')
axes[1].legend()
axes[1].grid(alpha=0.3)
axes[1].axhline(0, color='k', linestyle='-', alpha=0.3)

plt.tight_layout()
plt.savefig(config.VISUALIZATIONS_DIR / 'training_curves.png', dpi=150, bbox_inches='tight')
plt.show()

print(f"âœ… Saved to {config.VISUALIZATIONS_DIR / 'training_curves.png'}")

# Save history
with open(config.RESULTS_DIR / 'training_history.pkl', 'wb') as f:
    pickle.dump(history, f)

# Save final model
model.save_lora_weights(config.CHECKPOINTS_DIR / 'final_model')

print("="*70)
print("ðŸŽ‰ TASK 2.2 COMPLETE!")
print("="*70)
print(f"\nResults saved to: {config.TASK2_2_DIR}")
print(f"  - Best model: {config.CHECKPOINTS_DIR / 'best_model'}")
print(f"  - Training curves: {config.VISUALIZATIONS_DIR / 'training_curves.png'}")
print(f"  - History: {config.RESULTS_DIR / 'training_history.pkl'}")